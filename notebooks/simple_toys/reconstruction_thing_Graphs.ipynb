{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "make graphs such that there is a clear center graph  and try to reconstruct that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ipython Notebook init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from eden.util import configure_logging\n",
    "import logging\n",
    "configure_logging(logging.getLogger(),verbosity=1) # use 2 for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the problem.\n",
    "\n",
    "use bursi graphs.\n",
    "\n",
    "cluster them\n",
    "\n",
    "for one cluster, determine a center\n",
    "\n",
    "-> train regressor on distance to that center.\n",
    "\n",
    "see if we can reach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  bursi graphs: \n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from itertools import islice\n",
    "def get_graphs(dataset_fname='../../toolsdata/bursi.pos.gspan', size=200):\n",
    "    return  islice(gspan_to_eden(dataset_fname),size)\n",
    "\n",
    "\n",
    "# cluster \n",
    "from eden.graph import Vectorizer\n",
    "vec=Vectorizer()\n",
    "matrix=vec.transform(get_graphs())\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters=5\n",
    "clusterengine=KMeans(n_clusters=n_clusters)\n",
    "ids=clusterengine.fit_predict(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as dist\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html\n",
    "import copy\n",
    "\n",
    "# get biggest cluster and its center\n",
    "ids=list(ids)\n",
    "li=[]\n",
    "for i in range(n_clusters):\n",
    "    li.append((  ids.count(i),i ))\n",
    "li.sort(reverse=True)\n",
    "print li\n",
    "biggest_cluster=li[0][1]\n",
    "center = clusterengine.cluster_centers_[biggest_cluster]\n",
    "\n",
    "# get relevant graphs\n",
    "clustergraphs=[]\n",
    "restgraphs=[]\n",
    "graphs=list(  get_graphs() )\n",
    "for gid, cluster in enumerate(ids):\n",
    "    if cluster==biggest_cluster:\n",
    "        clustergraphs.append(graphs[gid])\n",
    "    else:\n",
    "        restgraphs.append(graphs[gid])\n",
    "    \n",
    "        \n",
    "def get_graphs():\n",
    "    # from here on out its only our new graphs\n",
    "    # making sure nobody destroys our beauty by deepcopying\n",
    "    return copy.deepcopy(clustergraphs)\n",
    "\n",
    "# get distance of all graphs to the centergraph\n",
    "matrix= vec.transform(get_graphs()) # transform may alter graphs\n",
    "closest = dist( matrix, center).argmin()\n",
    "distances_to_centergraph=dist(matrix,matrix[closest])\n",
    "maxdist = distances_to_centergraph.max()\n",
    "distances_to_centergraph = list(distances_to_centergraph.flatten())\n",
    "regression_targets=[ maxdist-dist for dist in distances_to_centergraph ]\n",
    "\n",
    "import graphlearn.utils.draw as draw\n",
    "draw.graphlearn(get_graphs()[closest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from graphlearn.graphlearn import  Sampler\n",
    "from graphlearn import estimate\n",
    "from eden.graph import Vectorizer\n",
    "\n",
    "# sample parameters\n",
    "n_steps=300 # how many steps\n",
    "n_samples=4 # collect this many samples during the process\n",
    "\n",
    "sampler=Sampler(\n",
    "                estimator=estimate.OneClassEstimator(nu=.7, cv=4, n_jobs=-1),\n",
    "                #estimator=estimate.Regressor(),\n",
    "                #vectorizer=Vectorizer(complexity=4),\n",
    "                #grammar=LocalSubstitutableGraphGrammar(radius_list=[0,1], thickness_list=[1,2], min_cip_count=2,min_interface_count=2),\n",
    "               \n",
    "                        n_steps=n_steps, n_samples=n_samples,\n",
    "                        probabilistic_core_choice=False, # if no opt are set, cores are chosen completely randomly...\n",
    "                        score_core_choice= False,\n",
    "                        size_diff_core_filter=12, # obviously we want to change the size\n",
    "                        burnin=10,\n",
    "                        include_seed=True,\n",
    "                        proposal_probability = False,\n",
    "                        improving_threshold=.5, \n",
    "                        improving_linear_start=0.0,\n",
    "                        accept_static_penalty=0.0,\n",
    "                        n_jobs=1,\n",
    "                        select_cip_max_tries=200,\n",
    "                        keep_duplicates=True,  \n",
    "                        monitor=True\n",
    "               \n",
    "               )\n",
    "sampler.fit(get_graphs())#using the regressor was a not so good idea..,regression_targets=regression_targets) # adding more stuff to the lsgg,lsgg_train_graphs=restgraphs)\n",
    "\n",
    "\n",
    "# lets look at a few stats about the trained sampler\n",
    "print('graph grammar stats:')\n",
    "n_instances, interface_counts, core_counts, cip_counts = sampler.grammar().size()\n",
    "print('#instances: %d   #interfaces: %d   #cores: %d   #core-interface-pairs: %d' % (n_instances, interface_counts, core_counts, cip_counts))\n",
    "\n",
    "# dumps the sampler for later use. This is not mandatory :) \n",
    "sampler.save('../../tmp/sampler.ge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Inspection\n",
    "\n",
    "If you are interested in the generated grammar, there are \n",
    "two useful tools available. \n",
    "You can draw the grammar directly, as seen below.\n",
    "draw_grammar_stats will collect statistics about the grammar and visualize them nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphlearn.utils.draw import draw_grammar\n",
    "# draw one group of graph fragments (CIPS)\n",
    "draw_grammar(sampler.grammar().productions, contract=True,\n",
    "             n_productions=7,\n",
    "             n_graphs_per_line=5,\n",
    "             n_graphs_per_production=5,\n",
    "             size=5, \n",
    "             colormap='rainbow', \n",
    "             node_border=1, \n",
    "             vertex_alpha=0.7, \n",
    "             edge_alpha=0.5, \n",
    "             node_size=700)\n",
    "#from graphlearn.utils.draw import draw_grammar_stats\n",
    "#draw_grammar_stats(sampler.lsgg.productions, size=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "\n",
    "Sampling with default options will work just fine if you are just interested\n",
    "in new graphs. The n_steps parameter defines \n",
    "how many attempts of altering a graph are made.\n",
    "\n",
    "##### Options that will influence the acceptance of a generated graph:\n",
    "\n",
    "In each sampling step, a graph is altered and scored.\n",
    "An accept function decides if we keep the new graph. The parameters listed here \n",
    "influence this acceptance function.\n",
    "\n",
    "improving_threshold=.5,\n",
    "after this fraction of steps, only improvements are accepted  ---\n",
    "improving_linear_start=0.0, \n",
    "graphs are accepted with a probability depending on their score. From this fraction it becomes gradually harder for worse graphs to be accepted. ---\n",
    "accept_static_penalty=0.0, \n",
    "graphs that are worse than their predecessors get this penalty (on top of the other two options).\n",
    "\n",
    "##### Options for choosing the new fragment:\n",
    "\n",
    "The fragment chosen for alteration can be influenced by the acceptable node parameter (see sampler init). \n",
    "In general it will be chosen randomly. The fragment it will be replaced with can be influenced however:\n",
    "\n",
    "probabilistic_core_choice=False, with this option we choose the fragment according to its frequency in the grammar.  ---\n",
    "score_core_choice= True, choose the fragment according to score ( given by estimator ), the better the score, the more likely it is for a fragment to be picked ---\n",
    "max_size_diff=1, maximum size difference between the seed graph and every graph generated graph. if choosing a fragment will violate the \n",
    "size constraints, it will not be selected.\n",
    "\n",
    "#####  Output multiple graphs (along the sample path):\n",
    "\n",
    "\n",
    "burnin=10, ignore the first burnin graphs for the nsamples parameter ---\n",
    "n_samples=n_samples, from burnin to end of sample, collect this many samples. ---\n",
    "keep_duplicates=True, duplicates may be deleted ---\n",
    "include_seed=True,  seed will be the first graph in the output list. \n",
    "\n",
    "##### Collect additional information during sampling, that may help debugging\n",
    "\n",
    "monitor=True, after sampling acessible via eg sampler.monitors[1][9] (first graph, ninth step)\n",
    "                        \n",
    "##### Output format \n",
    "sample() will yield a list of graph for each input graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from itertools import islice\n",
    "from graphlearn.graphlearn import  Sampler\n",
    "sampler=Sampler()\n",
    "sampler.load('../../tmp/sampler.ge')\n",
    "\n",
    "# picking graphs\n",
    "graphs = get_graphs()\n",
    "id_start=4\n",
    "id_end=id_start+12\n",
    "input_graphs = islice(graphs,id_start,id_end)\n",
    "\n",
    "\n",
    "\n",
    "graphs = sampler.transform(input_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "BABELDRAW=False\n",
    "# for each graphlist that is yielded by the sampler:\n",
    "scores=[]\n",
    "ids=range(id_start,id_end)\n",
    "for i,graphlist in enumerate(graphs):\n",
    "    \n",
    "    # collect scores of each graph that appeared during the sampling \n",
    "    print 'Graph id: %d'%(ids[i])\n",
    "    scores.append(sampler.monitors[i].sampling_info['score_history'])\n",
    "    \n",
    "    # choose a drawing method.\n",
    "    #print [(extract_label_sequence(g),compare(extract_label_sequence(g),ref,flength)) for g in graphlist ]\n",
    "    if BABELDRAW:\n",
    "        # babel draw looks nice, but may lack detail\n",
    "        from graphlearn.utils import openbabel\n",
    "        openbabel.draw(graphlist, d3=False, n_graphs_per_line=6,size=200)\n",
    "    else:\n",
    "        from graphlearn.utils import draw\n",
    "        # graphlearns drawing method is offering many options\n",
    "        draw.graphlearn(graphlist,\n",
    "                        contract=True,\n",
    "                        n_graphs_per_line=6, \n",
    "                        size=5, \n",
    "                        colormap='Paired', \n",
    "                        invert_colormap=False,\n",
    "                        node_border=0.5, \n",
    "                        vertex_color='_labels_',\n",
    "                        vertex_alpha=0.5, \n",
    "                        edge_alpha=0.2, \n",
    "                        node_size=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show sample score history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "markevery=n_steps/(n_samples)\n",
    "step=1\n",
    "num_graphs_per_plot=3\n",
    "num_plots=np.ceil([len(scores)/num_graphs_per_plot])\n",
    "\n",
    "for i in range(num_plots):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    for j,score in enumerate(scores[i*num_graphs_per_plot:i*num_graphs_per_plot+num_graphs_per_plot]):\n",
    "     \n",
    "        data = list(islice(score,None, None, step))\n",
    "        plt.plot(data, linewidth=2, label='graph %d'%(j+i*num_graphs_per_plot+id_start))\n",
    "        plt.plot(data, linestyle='None',markevery=markevery, markerfacecolor='white', marker='o', markeredgewidth=2,markersize=6)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.xlim(-1,n_steps+1)\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We collect Stats on the grammar\n",
    "\n",
    "first initialise logger :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from eden.util import configure_logging\n",
    "import logging\n",
    "configure_logging(logging.getLogger(),verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from itertools import islice\n",
    "def get_graphs(dataset_fname, size=100):\n",
    "    return  islice(gspan_to_eden(dataset_fname),size)\n",
    "\n",
    "dataset_fname = 'toolsdata/bursi.pos.gspan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### induce a grammar and train an estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from graphlearn.graphlearn import Sampler\n",
    "training_graphs = get_graphs(dataset_fname, size=200)\n",
    "sampler=Sampler(radius_list=[0,1],thickness_list=[2],\n",
    "                          min_cip_count=2, min_interface_count=2)\n",
    "sampler.fit(training_graphs,grammar_n_jobs=1, grammar_batch_size=10)\n",
    "\n",
    "print('graph grammar stats:')\n",
    "n_instances, n_interfaces, n_cores, n_cips = sampler.grammar().size()\n",
    "print('#instances: %d   #interfaces: %d   #cores: %d   #core-interface-pairs: %d' % (n_instances,\n",
    "                                                                                     n_interfaces,\n",
    "                                                                                     n_cores,\n",
    "                                                                                     n_cips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are the most frequent CIPS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#draw production rules\n",
    "from graphlearn.utils.draw import draw_grammar\n",
    "draw_grammar(sampler.grammar().productions, contract=True,\n",
    "             n_productions=6,n_graphs_per_line=6, \n",
    "             size=5, colormap='rainbow', node_border=1, vertex_alpha=0.7, edge_alpha=0.5, node_size=700)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how are the CIPs distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grammar statistics\n",
    "from graphlearn.utils.draw import draw_grammar_stats\n",
    "draw_grammar_stats(sampler.lsgg.productions, size=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse grammar size under different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from graphlearn.graphlearn import Sampler\n",
    "from itertools import tee\n",
    "for radius in range(0,2):\n",
    "    for thickness in range(1,3):\n",
    "        print 'Radius: %d   Thickness: %d' % (radius, thickness)\n",
    "        for size in range(100,250,50):\n",
    "            training_graphs = get_graphs(dataset_fname, size=size)\n",
    "            training_graphs,training_graphs_=tee(training_graphs)\n",
    "\n",
    "            sampler=Sampler(radius_list=[radius],\n",
    "                                      thickness_list=[thickness],\n",
    "                                      min_cip_count=2, \n",
    "                                      min_interface_count=2,\n",
    "                                      random_state=42)\n",
    "\n",
    "            sampler.fit(training_graphs_,grammar_n_jobs=1, grammar_batch_size=10)\n",
    "\n",
    "            n_instances, n_interfaces, n_cores, n_cips = sampler.lsgg.size()\n",
    "            print('#instances: %3d   #interfaces: %4d   #cores: %4d   #core-interface-pairs: %5d' % (n_instances,\n",
    "                                                                                                 n_interfaces,\n",
    "                                                                                                 n_cores,\n",
    "                                                                                                 n_cips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
